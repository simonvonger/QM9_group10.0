{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from Dataset import DataSetQM9\n",
    "\n",
    "#TODO: overvej at bruger nworkers\n",
    "\n",
    "class DataLoaderQM9(DataLoader):\n",
    "    def __init__(self, datapath: str = \"data\", batchsize: int = 50, r_cut: float = 5., self_edge: bool=False,test_split: float = 0.1,val_split: float=0.2, nworkers: int = 2):\n",
    "        self.r_cut= r_cut\n",
    "        self.dataset = DataSetQM9(path=datapath, r_cut=r_cut,self_edge=self_edge)\n",
    "        self.length=len(self.dataset)\n",
    "        self.train_sampler = SubsetRandomSampler(np.array(range(self.length)))\n",
    "        self.valid_sampler = None\n",
    "        self.test_sampler = None\n",
    "        # self.batchsize = batchsize\n",
    "\n",
    "        if test_split:\n",
    "            self.test_sampler = self._split(test_split)\n",
    "        if val_split:\n",
    "            self.test_sampler = self._split(val_split)\n",
    "        self.init_kwargs = {'batchsize': batchsize, 'num_workers': nworkers} #TODO: overvej nworkers\n",
    "        #Return training set\n",
    "        super().__init__(self.dataset, sampler=self.train_sampler, collate_fn=self.collate_fn, **self.init_kwargs)\n",
    "    \n",
    "    \n",
    "    def collate_fn(self, data):\n",
    "        \"\"\"Handle how we stack a batch\n",
    "        Args:\n",
    "            data: the data before we output the batch (a tuple containing the dictionary for each molecule)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_dict = {k: [dic[k] for dic in data] for k in data[0].keys()} \n",
    "\n",
    "        # We need to define the id and the edges_coord differently (because we begin indexing from 0)\n",
    "        n_atoms = torch.tensor(batch_dict[\"n_atom\"])\n",
    "        \n",
    "        # Converting the n_atom into unique id\n",
    "        ids = torch.repeat_interleave(torch.tensor(range(len(batch_dict['n_atom']))), n_atoms)\n",
    "        # Adding the offset to the neighbours coordinate\n",
    "        edges_coord = torch.cumsum(torch.cat((torch.tensor([0]), n_atoms[:-1])), dim=0)\n",
    "        neighbours = torch.tensor([local_neigh.shape[0] for local_neigh in batch_dict['edges']])\n",
    "        edges_coord = torch.cat([torch.repeat_interleave(edges_coord, neighbours).unsqueeze(dim=1), torch.repeat_interleave(edges_coord, neighbours).unsqueeze(dim=1)], dim=1)\n",
    "        edges_coord += torch.cat(batch_dict['edges'])\n",
    "\n",
    "        return {\n",
    "            'z': torch.cat(batch_dict['z']),\n",
    "            'xyz': torch.cat(batch_dict['xyz']),\n",
    "            'edges': edges_coord,\n",
    "            'r_ij': torch.cat(batch_dict['r_ij']),\n",
    "            'r_ij_normalized': torch.cat(batch_dict['r_ij_normalized']),\n",
    "            'graph_idx': ids,\n",
    "            'targets': torch.cat(batch_dict['targets'])\n",
    "        }\n",
    "    def _split(self, validation_split: float):\n",
    "        \"\"\" Creates a sampler to extract training and validation data\n",
    "        Args:\n",
    "            validation_split: decimal for the split of the validation\n",
    "        \"\"\"    \n",
    "        train_idx = np.array(range(self.length))\n",
    "\n",
    "        # Getting randomly the index of the validation split (we therefore don't need to shuffle)\n",
    "        split_idx = np.random.choice(\n",
    "            train_idx, \n",
    "            int(self.length*validation_split), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # Deleting the corresponding index in the training set\n",
    "        train_idx = np.delete(train_idx, split_idx)\n",
    "\n",
    "        # Getting the corresponding PyTorch samplers\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        self.train_sampler = train_sampler\n",
    "\n",
    "        return SubsetRandomSampler(split_idx)\n",
    "\n",
    "    def get_val(self) -> list:\n",
    "        \"\"\" Return the validation data\"\"\"\n",
    "        if self.valid_sampler is None:\n",
    "            return None\n",
    "        else: \n",
    "            return DataLoader(self.dataset, sampler=self.valid_sampler, collate_fn=self.collate_fn, **self.init_kwargs)\n",
    "\n",
    "    def get_test(self) -> list:\n",
    "        \"\"\" Return the test data\"\"\"\n",
    "        if self.test_sampler is None:\n",
    "            return None\n",
    "        else: \n",
    "            return DataLoader(self.dataset, sampler=self.test_sampler, collate_fn = self.collate_fn, **self.init_kwargs)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
