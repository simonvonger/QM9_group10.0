{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu will be used for training the PaiNN model\n",
      "Current loss 100.92992401123047 Current batch 0/3271 (0.00%)\n",
      "Current loss 1.1880638599395752 Current batch 100/3271 (3.06%)\n",
      "Current loss 0.5633002519607544 Current batch 200/3271 (6.11%)\n",
      "Current loss 0.15038716793060303 Current batch 300/3271 (9.17%)\n",
      "Current loss 0.3294963240623474 Current batch 400/3271 (12.23%)\n",
      "Current loss 0.22582776844501495 Current batch 500/3271 (15.29%)\n",
      "Current loss 0.15274913609027863 Current batch 600/3271 (18.34%)\n",
      "Current loss 0.29093632102012634 Current batch 700/3271 (21.40%)\n",
      "Current loss 0.2995820641517639 Current batch 800/3271 (24.46%)\n",
      "Current loss 0.38052186369895935 Current batch 900/3271 (27.51%)\n",
      "Current loss 0.1903200000524521 Current batch 1000/3271 (30.57%)\n",
      "Current loss 0.3494141697883606 Current batch 1100/3271 (33.63%)\n",
      "Current loss 0.5218707919120789 Current batch 1200/3271 (36.69%)\n",
      "Current loss 1.0107357501983643 Current batch 1300/3271 (39.74%)\n",
      "Current loss 0.2779139578342438 Current batch 1400/3271 (42.80%)\n",
      "Current loss 0.20113447308540344 Current batch 1500/3271 (45.86%)\n",
      "Current loss 0.1424301415681839 Current batch 1600/3271 (48.91%)\n",
      "Current loss 0.23959967494010925 Current batch 1700/3271 (51.97%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         trainer\u001b[39m.\u001b[39mplot_data()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     training()\n",
      "\u001b[1;32m/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     Model\u001b[39m=\u001b[39mModel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss\u001b[39m=\u001b[39mmse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     device\u001b[39m=\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39m_train(num_epoch \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, early_stopping \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hansmoeller/Documents/GitHub/QM9_group10.0/Main.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39mplot_data()\n",
      "File \u001b[0;32m~/Documents/GitHub/QM9_group10.0/Training.py:142\u001b[0m, in \u001b[0;36mTrainer._train\u001b[0;34m(self, num_epoch, early_stopping, alpha)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m#min_loss = float('inf')  # Initialize min_loss with a large value\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epoch):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_epoch()\n\u001b[1;32m    143\u001b[0m     \u001b[39m# Validate at the end of an epoch\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_model()\n",
      "File \u001b[0;32m~/Documents/GitHub/QM9_group10.0/Training.py:61\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m targets \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m\"\u001b[39m][:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[39m# Backpropagate using the selected loss\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mModel(batch)\n\u001b[1;32m     62\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(outputs, targets)\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m batch_num\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/QM9_group10.0/Model.py:84\u001b[0m, in \u001b[0;36mPaiNN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     78\u001b[0m v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((unique_atm_mat\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_size), \u001b[39m# tidligere navn: v_j\u001b[39;00m\n\u001b[1;32m     79\u001b[0m                           device \u001b[39m=\u001b[39m r_ij\u001b[39m.\u001b[39mdevice,\n\u001b[1;32m     80\u001b[0m                           dtype \u001b[39m=\u001b[39m r_ij\u001b[39m.\u001b[39mdtype\n\u001b[1;32m     81\u001b[0m                           )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m message_block, update_block \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessage_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_blocks):\n\u001b[0;32m---> 84\u001b[0m     s, v \u001b[39m=\u001b[39m message_block(\n\u001b[1;32m     85\u001b[0m         s \u001b[39m=\u001b[39m s,\n\u001b[1;32m     86\u001b[0m         v \u001b[39m=\u001b[39m v,\n\u001b[1;32m     87\u001b[0m         edges \u001b[39m=\u001b[39m edges,\n\u001b[1;32m     88\u001b[0m         r_ij \u001b[39m=\u001b[39m r_ij,\n\u001b[1;32m     89\u001b[0m         r_ij_normalized \u001b[39m=\u001b[39m r_ij_normalized\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     s, v \u001b[39m=\u001b[39m update_block(\n\u001b[1;32m     92\u001b[0m         s \u001b[39m=\u001b[39m s,\n\u001b[1;32m     93\u001b[0m         v \u001b[39m=\u001b[39m v\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     96\u001b[0m blue_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblue_block(s)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/QM9_group10.0/Model.py:140\u001b[0m, in \u001b[0;36mMessageBlock.forward\u001b[0;34m(self, s, v, edges, r_ij, r_ij_normalized)\u001b[0m\n\u001b[1;32m    136\u001b[0m f_cut \u001b[39m=\u001b[39m fcut(r_ij, r_cut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr_cut)\n\u001b[1;32m    138\u001b[0m rbf_fcut \u001b[39m=\u001b[39m rbf_pass \u001b[39m*\u001b[39m f_cut\n\u001b[0;32m--> 140\u001b[0m s_pass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(s)\n\u001b[1;32m    142\u001b[0m pass_out \u001b[39m=\u001b[39m rbf_fcut \u001b[39m*\u001b[39m s_pass[edges[:,\u001b[39m1\u001b[39m]]\n\u001b[1;32m    144\u001b[0m delta_v, delta_s, delta_rep \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(pass_out, \u001b[39m128\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "from Dataloader import DataLoaderQM9\n",
    "from Model import PaiNN\n",
    "from Training import Trainer\n",
    "from Model import mse, mae\n",
    "\n",
    "def training():\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"{device} will be used for training the PaiNN model\")\n",
    "        Model = PaiNN(r_cut=5, \n",
    "                device=device\n",
    "                ).to(device)\n",
    "\n",
    "        train_set = DataLoaderQM9(r_cut=5,batch_size=100)\n",
    "        optimizer = torch.optim.Adam(params=Model.parameters(), lr = 5e-4, weight_decay = 0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience = 5)\n",
    "        trainer = Trainer(\n",
    "            Model=Model,\n",
    "            loss=mse,\n",
    "            target=2,\n",
    "            optimizer=optimizer,\n",
    "            Dataloader=train_set,\n",
    "            scheduler=scheduler,\n",
    "            device=device\n",
    "        )\n",
    "        trainer._train(num_epoch = 100, early_stopping = 30)\n",
    "        trainer.plot_data()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
